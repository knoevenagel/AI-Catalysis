{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "from autogluon.tabular import  TabularPredictor\n",
    "from greedy_deletion import greedy_delete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('example/dummy_-1_1_scaled.csv',index_col = 'information')\n",
    "data_iter = data\n",
    "train_data = data.sample(n = int(0.7*len(data)))\n",
    "test_data = data.drop(train_data.index)\n",
    "train_data_index = list(train_data.index)\n",
    "test_data_index = list(test_data.index)\n",
    "train_data_pd = pd.DataFrame(train_data_index)\n",
    "test_data_pd = pd.DataFrame(test_data_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230925_022047\\\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230925_022047\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22000\n",
      "Train Data Rows:    5892\n",
      "Train Data Columns: 139\n",
      "Label Column: Adsorption Energy\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.17733797, -12.89793472, -3.24352, 3.69822)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6953.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 74 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['LUMO', 'HOMO', '298K_Atomization_Energy', 'Diatomic', 'Bond_length', ...]\n",
      "\t\t('int', [])   : 74 | ['class_transition_A', 'class_covalent_A', 'class_nonmetal_A', 'class_transition_B', 'class_covalent_B', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 65 | ['LUMO', 'HOMO', '298K_Atomization_Energy', 'Diatomic', 'Bond_length', ...]\n",
      "\t\t('int', ['bool']) : 74 | ['class_transition_A', 'class_covalent_A', 'class_nonmetal_A', 'class_transition_B', 'class_covalent_B', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t139 features in original data used to generate 139 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.5 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.3s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5302, Val Rows: 590\n",
      "Excluded Model Types: ['GBM', 'CAT', 'RF', 'XT', 'LR', 'KNN', 'NN_MXNET', 'NN_TORCH', 'FASTAI']\n",
      "\tFound 'NN_TORCH' model in hyperparameters, but 'NN_TORCH' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'CAT' model in hyperparameters, but 'CAT' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'FASTAI' model in hyperparameters, but 'FASTAI' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'RF' model in hyperparameters, but 'RF' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'XT' model in hyperparameters, but 'XT' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'XT' model in hyperparameters, but 'XT' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'XT' model in hyperparameters, but 'XT' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\science\\Autogulon\\final_codes\\pack_functions\\pack_functions_v2\\greedy_deletion_example.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_test_rate \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#unfix train set and test set\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#greedy_delete(data,feature_list,label,train_model,train_test_rate,mae_path,feature_path)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#fix train set and test set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/science/Autogulon/final_codes/pack_functions/pack_functions_v2/greedy_deletion_example.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m greedy_delete(data,feature_list,label,train_model,train_test_rate,mae_path,feature_path,train_data_index \u001b[39m=\u001b[39;49mtrain_data_index ,test_data_index \u001b[39m=\u001b[39;49m test_data_index)\n",
      "File \u001b[1;32me:\\science\\Autogulon\\final_codes\\pack_functions\\pack_functions_v2\\greedy_deletion.py:61\u001b[0m, in \u001b[0;36mgreedy_delete\u001b[1;34m(data, feature_list, label, train_model, train_test_rate, mae_path, feature_path, train_data_index, test_data_index, model_path)\u001b[0m\n\u001b[0;32m     59\u001b[0m train_data \u001b[39m=\u001b[39m data_iter\u001b[39m.\u001b[39mloc[train_data_index]\n\u001b[0;32m     60\u001b[0m test_data \u001b[39m=\u001b[39m data_iter\u001b[39m.\u001b[39mloc[test_data_index]\n\u001b[1;32m---> 61\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49mlabel,eval_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m,path \u001b[39m=\u001b[39;49m model_path)\\\n\u001b[0;32m     62\u001b[0m         \u001b[39m.\u001b[39;49mfit(train_data,excluded_model_types \u001b[39m=\u001b[39;49m excluded_model_types)  \u001b[39m# Fit models for 120s\u001b[39;00m\n\u001b[0;32m     63\u001b[0m leaderboard \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mleaderboard(test_data,extra_metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmean_absolute_error\u001b[39m\u001b[39m'\u001b[39m],silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m record_mae \u001b[39m=\u001b[39m record_mae \u001b[39m+\u001b[39m [\u001b[39m-\u001b[39mleaderboard[\u001b[39m'\u001b[39m\u001b[39mmean_absolute_error\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     29\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39mgargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:866\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m     aux_kwargs[\u001b[39m'\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m--> 866\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mtrain_data, X_val\u001b[39m=\u001b[39;49mtuning_data, X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[0;32m    867\u001b[0m                   holdout_frac\u001b[39m=\u001b[39;49mholdout_frac, num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds, num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[0;32m    868\u001b[0m                   num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    869\u001b[0m                   hyperparameters\u001b[39m=\u001b[39;49mhyperparameters, core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs, aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    870\u001b[0m                   time_limit\u001b[39m=\u001b[39;49mtime_limit, infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    871\u001b[0m                   verbosity\u001b[39m=\u001b[39;49mverbosity, use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout)\n\u001b[0;32m    872\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[0;32m    875\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    876\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m    881\u001b[0m )\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:125\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:118\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m--> 118\u001b[0m trainer\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    119\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    120\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[0;32m    121\u001b[0m     X_val\u001b[39m=\u001b[39mX_val,\n\u001b[0;32m    122\u001b[0m     y_val\u001b[39m=\u001b[39my_val,\n\u001b[0;32m    123\u001b[0m     X_unlabeled\u001b[39m=\u001b[39mX_unlabeled,\n\u001b[0;32m    124\u001b[0m     holdout_frac\u001b[39m=\u001b[39mholdout_frac,\n\u001b[0;32m    125\u001b[0m     time_limit\u001b[39m=\u001b[39mtime_limit_trainer,\n\u001b[0;32m    126\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m    127\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    128\u001b[0m     groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    129\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_fit_kwargs\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m    132\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:98\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_bag_holdout:\n\u001b[0;32m     86\u001b[0m         \u001b[39m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         \u001b[39m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         \u001b[39m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         \u001b[39m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mX_val, y_val is not None, but bagged mode was specified. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     91\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mIf calling from `TabularPredictor.fit()`, `tuning_data` should be None.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     92\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mDefault bagged mode does not use tuning data / validation data. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mspecify the following:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     96\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m     99\u001b[0m                                y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    100\u001b[0m                                X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    101\u001b[0m                                y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    102\u001b[0m                                X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    103\u001b[0m                                hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    104\u001b[0m                                num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    105\u001b[0m                                time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    106\u001b[0m                                core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    107\u001b[0m                                aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    108\u001b[0m                                infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    109\u001b[0m                                infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    110\u001b[0m                                groups\u001b[39m=\u001b[39;49mgroups)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[0;32m   2050\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m-> 2051\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_multi_levels(X, y, hyperparameters\u001b[39m=\u001b[39mhyperparameters, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val,\n\u001b[0;32m   2052\u001b[0m                                           X_unlabeled\u001b[39m=\u001b[39mX_unlabeled, level_start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, level_end\u001b[39m=\u001b[39mnum_stack_levels\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, time_limit\u001b[39m=\u001b[39mtime_limit, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2054\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:312\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    310\u001b[0m         core_kwargs_level[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m, time_limit_core)\n\u001b[0;32m    311\u001b[0m         aux_kwargs_level[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 312\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[0;32m    313\u001b[0m         X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    314\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters, level\u001b[39m=\u001b[39;49mlevel, base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    315\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level, aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level, name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[0;32m    316\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    317\u001b[0m     )\n\u001b[0;32m    318\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[0;32m    319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:429\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    427\u001b[0m     core_kwargs[\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m    428\u001b[0m     aux_kwargs[\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m--> 429\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_core(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val, X_unlabeled\u001b[39m=\u001b[39mX_unlabeled, models\u001b[39m=\u001b[39mmodels,\n\u001b[0;32m    430\u001b[0m                                         level\u001b[39m=\u001b[39mlevel, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, base_model_names\u001b[39m=\u001b[39mbase_model_names, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcore_kwargs)\n\u001b[0;32m    432\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    434\u001b[0m                                           infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:520\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    517\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m    519\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_multi(X\u001b[39m=\u001b[39mX_init, y\u001b[39m=\u001b[39my, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val, X_unlabeled\u001b[39m=\u001b[39mX_unlabeled,\n\u001b[0;32m    521\u001b[0m                          models\u001b[39m=\u001b[39mmodels, level\u001b[39m=\u001b[39mlevel, stack_name\u001b[39m=\u001b[39mstack_name, compute_score\u001b[39m=\u001b[39mcompute_score, fit_kwargs\u001b[39m=\u001b[39mfit_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2021\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2019\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2020\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 2021\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_multi_initial(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, models\u001b[39m=\u001b[39mmodels, k_fold\u001b[39m=\u001b[39mk_fold, n_repeats\u001b[39m=\u001b[39mn_repeats_initial, hyperparameter_tune_kwargs\u001b[39m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   2022\u001b[0m                                                     feature_prune_kwargs\u001b[39m=\u001b[39mfeature_prune_kwargs, time_limit\u001b[39m=\u001b[39mtime_limit, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2023\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2024\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1913\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bagged:\n\u001b[0;32m   1912\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1913\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_multi_fold(models\u001b[39m=\u001b[39mmodels, hyperparameter_tune_kwargs\u001b[39m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   1914\u001b[0m                                     time_limit\u001b[39m=\u001b[39mtime_limit, time_split\u001b[39m=\u001b[39mtime_split, time_ratio\u001b[39m=\u001b[39mtime_ratio, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_args)\n\u001b[0;32m   1915\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1916\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1992\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1990\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1991\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[1;32m-> 1992\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single_full(X, y, model, time_limit\u001b[39m=\u001b[39mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m   1995\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1810\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start)\n\u001b[0;32m   1809\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 1810\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_and_save(\n\u001b[0;32m   1811\u001b[0m         X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   1812\u001b[0m         y\u001b[39m=\u001b[39my,\n\u001b[0;32m   1813\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   1814\u001b[0m         X_val\u001b[39m=\u001b[39mX_val,\n\u001b[0;32m   1815\u001b[0m         y_val\u001b[39m=\u001b[39my_val,\n\u001b[0;32m   1816\u001b[0m         X_unlabeled\u001b[39m=\u001b[39mX_unlabeled,\n\u001b[0;32m   1817\u001b[0m         stack_name\u001b[39m=\u001b[39mstack_name,\n\u001b[0;32m   1818\u001b[0m         level\u001b[39m=\u001b[39mlevel,\n\u001b[0;32m   1819\u001b[0m         compute_score\u001b[39m=\u001b[39mcompute_score,\n\u001b[0;32m   1820\u001b[0m         total_resources\u001b[39m=\u001b[39mtotal_resources,\n\u001b[0;32m   1821\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs\n\u001b[0;32m   1822\u001b[0m     )\n\u001b[0;32m   1823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m   1824\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1502\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39mtotal_resources, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1504\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1447\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[0;32m   1443\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1444\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1447\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val, total_resources\u001b[39m=\u001b[39mtotal_resources, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1448\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:695\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39mFit model to predict values in y based on X.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39m    Any additional fit arguments a model supports.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    694\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# FIXME: This might have to go before self._preprocess_fit_args, but then time_limit might be incorrect in **kwargs init to initialize\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_fit_args(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    697\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mWarning: Model has no time left to train, skipping model... (Time Left = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39ms)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:428\u001b[0m, in \u001b[0;36mAbstractModel._preprocess_fit_args\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     time_limit \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(time_limit, min_time_limit)\n\u001b[0;32m    427\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m time_limit\n\u001b[1;32m--> 428\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m kwargs\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:515\u001b[0m, in \u001b[0;36mAbstractModel._preprocess_fit_resources\u001b[1;34m(self, silent, total_resources, parallel_hpo, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m kwargs\n\u001b[0;32m    514\u001b[0m system_num_cpus \u001b[39m=\u001b[39m ResourceManager\u001b[39m.\u001b[39mget_cpu_count()\n\u001b[1;32m--> 515\u001b[0m system_num_gpus \u001b[39m=\u001b[39m ResourceManager\u001b[39m.\u001b[39;49mget_gpu_count_all()\n\u001b[0;32m    516\u001b[0m \u001b[39mif\u001b[39;00m total_resources \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     total_resources \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\common\\utils\\resource_utils.py:31\u001b[0m, in \u001b[0;36mResourceManager.get_gpu_count_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     num_gpus \u001b[39m=\u001b[39m ResourceManager\u001b[39m.\u001b[39mget_gpu_count_mxnet()\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m num_gpus \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m         num_gpus \u001b[39m=\u001b[39m ResourceManager\u001b[39m.\u001b[39;49mget_gpu_count_torch()\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m num_gpus\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\autogluon\\common\\utils\\resource_utils.py:47\u001b[0m, in \u001b[0;36mResourceManager.get_gpu_count_torch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_gpu_count_torch\u001b[39m():\n\u001b[0;32m     46\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         num_gpus \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\torch\\__init__.py:121\u001b[0m\n\u001b[0;32m    119\u001b[0m is_loaded \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 121\u001b[0m     res \u001b[39m=\u001b[39m kernel32\u001b[39m.\u001b[39;49mLoadLibraryExW(dll, \u001b[39mNone\u001b[39;49;00m, \u001b[39m0x00001100\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m     last_error \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mget_last_error()\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m last_error \u001b[39m!=\u001b[39m \u001b[39m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_list = ['dbcenter_B','neighbor_B_3','outer_d_B_9']\n",
    "label = 'Adsorption Energy'\n",
    "train_model = 'XGB'\n",
    "mae_path = 'example/res/mae.csv'\n",
    "feature_path = 'example/res/feature.csv'\n",
    "train_test_rate = 0.7\n",
    "\n",
    "#unfix train set and test set\n",
    "#greedy_delete(data,feature_list,label,train_model,train_test_rate,mae_path,feature_path)\n",
    "\n",
    "#fix train set and test set\n",
    "greedy_delete(data,feature_list,label,train_model,train_test_rate,mae_path,feature_path,train_data_index =train_data_index ,test_data_index = test_data_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
